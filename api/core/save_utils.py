#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import re
import markdownify
import base64
import requests
import shutil
import zipfile
import sys
import tempfile
import platform
import html2text
import pdfkit
import random
import string
import quopri

from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from datetime import datetime
from io import BytesIO
from .log_utils import logger

def get_save_path(file_name, file_path):
    # æ‹¼æ¥æ–‡ä»¶è·¯å¾„
    filepath = os.path.join(file_path, file_name)
    # ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    return filepath


def base_save_handle(title, content, css_styles, file_name, file_path, base_url=None, platform=None):
    # å¤„ç†å›¾ç‰‡
    logger.info("base_save_handle å¤„ç†å›¾ç‰‡" + base_url)
    logger.info(f"contentç±»å‹: {type(content)}")
    if base_url:
        logger.info("base_save_handle å¤„ç†å›¾ç‰‡00" + base_url)
        content = process_images_in_content(content, base_url, file_path)
        
    # å¦‚æœcontentæ˜¯BeautifulSoupå¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    if isinstance(content, BeautifulSoup):
        content = str(content)

    return content

def create_html_template(title, content, css_styles, base_url=None, platform=None):
    # åˆ›å»ºHTMLæ¨¡æ¿
    html_template = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <title>{title}</title>
        <style>
            {css_styles}
        </style>
    </head>
    <body>
        <article>
            <h1><a href={base_url}>{title}</a></h1>
            {content}
        </article>
    </body>
    </html>
    """
    return html_template
    
def save_as_html(title, content, css_styles, file_name, file_path, base_url=None, platform=None):
    """å°†åšå®¢å†…å®¹ä¿å­˜ä¸ºHTMLæ ¼å¼
    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« å†…å®¹
        css_styles: CSSæ ·å¼å†…å®¹
        file_name: æ–‡ä»¶å
        file_path: ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
        base_url: åŸå§‹é¡µé¢çš„URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
        platform: å¹³å°åç§°ï¼Œç”¨äºåŠ è½½ç‰¹å®šçš„CSSæ ·å¼
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        filepath = get_save_path(file_name, file_path)
        content = base_save_handle(title, content, css_styles, file_name, file_path, base_url, platform)
            
        # åˆ›å»ºHTMLæ¨¡æ¿
        html_template = create_html_template(title, content, css_styles, base_url, platform)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_template)
            
        logger.info(f"HTMLæ–‡ä»¶å·²ä¿å­˜: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"ä¿å­˜HTMLæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def save_as_markdown(title, content, css_styles, file_name, file_path, base_url=None, platform=None):
    """å°†åšå®¢å†…å®¹ä¿å­˜ä¸ºMarkdownæ ¼å¼
    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« å†…å®¹
        css_styles: CSSæ ·å¼å†…å®¹
        file_name: æ–‡ä»¶å
        file_path: ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
        base_url: åŸå§‹é¡µé¢çš„URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
        platform: å¹³å°åç§°ï¼Œç”¨äºåŠ è½½ç‰¹å®šçš„CSSæ ·å¼
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        filepath = get_save_path(file_name, file_path)
        content = base_save_handle(title, content, css_styles, file_name, file_path, base_url, platform)
            
        # å°†HTMLè½¬æ¢ä¸ºMarkdown
        markdown_converter = html2text.HTML2Text()
        markdown_converter.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
        markdown_content = markdown_converter.handle(content)
        
        # æ·»åŠ æ ‡é¢˜
        final_content = f"# {title}\n\n{markdown_content}"
        
        # ä¿å­˜Markdownæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(final_content)
            
        logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"ä¿å­˜Markdownæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def get_wkhtmltopdf_path() -> str:
    """
    è·å– wkhtmltopdf å¯æ‰§è¡Œæ–‡ä»¶çš„è·¯å¾„
    Linux ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ç‰ˆæœ¬ï¼ŒWindows ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ç‰ˆæœ¬
    """
    if platform.system() == 'Windows':
        # Windows ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ç‰ˆæœ¬
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        wkhtmltopdf_path = os.path.join(base_dir, 'tools', 'wkhtmltopdf', 'bin', 'wkhtmltopdf.exe')
        if not os.path.exists(wkhtmltopdf_path):
            raise FileNotFoundError(f'wkhtmltopdf not found at {wkhtmltopdf_path}')
        return wkhtmltopdf_path
    else:
        # Linux ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ç‰ˆæœ¬
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            '/usr/local/bin/wkhtmltopdf',
            '/usr/bin/wkhtmltopdf',
            'wkhtmltopdf'  # å¦‚æœåœ¨ PATH ä¸­
        ]
        for path in possible_paths:
            if os.path.exists(path):
                return path
            # å¯¹äº 'wkhtmltopdf'ï¼Œä½¿ç”¨ which å‘½ä»¤æ£€æŸ¥
            elif path == 'wkhtmltopdf':
                try:
                    import subprocess
                    result = subprocess.run(['which', 'wkhtmltopdf'], capture_output=True, text=True)
                    if result.returncode == 0:
                        return 'wkhtmltopdf'
                except:
                    pass
        raise FileNotFoundError('wkhtmltopdf not found in system')

def save_as_pdf(title, content, css_styles, file_name, file_path, base_url=None, platform=None):
    """å°†åšå®¢å†…å®¹ä¿å­˜ä¸ºPDFæ ¼å¼"""
    try:
        # 1. å…ˆä¿å­˜ä¸ºä¸´æ—¶HTMLæ–‡ä»¶
        temp_html = os.path.join(file_path, f"{os.path.splitext(file_name)[0]}_temp.html")
        with open(temp_html, 'w', encoding='utf-8') as f:
            html_content = create_html_template(title, content, css_styles, base_url, platform)
            f.write(html_content)
        
        # 2. é…ç½®wkhtmltopdfé€‰é¡¹
        options = {
            'enable-local-file-access': None,  # å…è®¸è®¿é—®æœ¬åœ°æ–‡ä»¶
            'encoding': 'utf-8',
            #'no-images': None,  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œé¿å…ç½‘ç»œé—®é¢˜
            #'disable-javascript': None  # ç¦ç”¨JavaScript
        }
        
        # 3. ç”ŸæˆPDFæ–‡ä»¶è·¯å¾„
        pdf_path = os.path.join(file_path, file_name)
        
        try:
            # 4. ä½¿ç”¨wkhtmltopdfè½¬æ¢
            config = pdfkit.configuration(wkhtmltopdf=get_wkhtmltopdf_path())
            pdfkit.from_file(temp_html, pdf_path, options=options, configuration=config)
            
            # 5. åˆ é™¤ä¸´æ—¶HTMLæ–‡ä»¶
            os.remove(temp_html)
            return pdf_path
            
        except Exception as e:
            logger.error(f"PDFè½¬æ¢å¤±è´¥: {str(e)}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä»HTMLå­—ç¬¦ä¸²è½¬æ¢
            try:
                pdfkit.from_string(html_content, pdf_path, options=options, configuration=config)
                return pdf_path
            except Exception as e2:
                logger.error(f"ä»å­—ç¬¦ä¸²è½¬æ¢PDFä¹Ÿå¤±è´¥: {str(e2)}")
                return None
                
    except Exception as e:
        logger.error(f"ä¿å­˜PDFæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def save_as_mhtml(title, content, css_styles, file_name, file_path, base_url=None, platform=None):
    """å°†HTMLå†…å®¹ä¿å­˜ä¸ºMHTMLæ ¼å¼
    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« å†…å®¹
        css_styles: CSSæ ·å¼å†…å®¹
        file_name: æ–‡ä»¶å
        file_path: ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„
        base_url: åŸå§‹é¡µé¢çš„URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„
        platform: å¹³å°åç§°ï¼Œç”¨äºåŠ è½½ç‰¹å®šçš„CSSæ ·å¼
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        # ç¼–ç HTMLå†…å®¹
        filepath = get_save_path(file_name, file_path)
        content = base_save_handle(title, content, css_styles, file_name, file_path, base_url, platform)
        html_content = create_html_template(title, content, css_styles, base_url, platform)

        # å¤„ç†å›¾ç‰‡
        images = handle_mhtml_images(content, base_url)

        # ç”ŸæˆMHTMLå¤´éƒ¨
        boundary = '----=_NextPart_' + ''.join(random.choices(string.ascii_letters + string.digits, k=16))
        mhtml_header = (
            f'From: <Saved by BlogTest>\n'
            f'Subject: {title}\n'
            f'Date: {datetime.now().strftime("%a, %d %b %Y %H:%M:%S %z")}\n'
            f'MIME-Version: 1.0\n'
            f'Content-Type: multipart/related;\n'
            f'\tboundary="{boundary}"\n\n'
        )

        # ä¿å­˜MHTMLæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            # 1. å†™å…¥MHTMLå¤´éƒ¨
            f.write(mhtml_header)
            
            # 2. å†™å…¥HTMLå†…å®¹éƒ¨åˆ†
            f.write(f'--{boundary}\n')
            f.write('Content-Type: text/html; charset="utf-8"\n')
            f.write('Content-Transfer-Encoding: quoted-printable\n')
            if base_url:
                f.write(f'Content-Location: {base_url}\n')
            f.write('\n')  # ç©ºè¡Œå¾ˆé‡è¦
            f.write(quopri.encodestring(html_content.encode('utf-8')).decode('utf-8'))
            
            # 3. å†™å…¥å›¾ç‰‡å†…å®¹
            for img in images:
                f.write(f'\n--{boundary}\n')
                f.write(f'Content-Type: {img["content_type"]}\n')
                f.write('Content-Transfer-Encoding: base64\n')
                f.write(f'Content-Location: {img["src"]}\n')
                f.write('\n')  # ç©ºè¡Œå¾ˆé‡è¦
                f.write(img['data'])
            
            # 4. å†™å…¥ç»“æŸæ ‡è®°
            f.write(f'\n--{boundary}--\n')
            
        logger.info(f"MHTMLæ–‡ä»¶å·²ä¿å­˜: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"ä¿å­˜MHTMLæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def handle_mhtml_images(content, base_url):
    # å¤„ç†å›¾ç‰‡å¹¶æ”¶é›†å›¾ç‰‡ä¿¡æ¯
    if isinstance(content, str):
        soup = BeautifulSoup(content, 'html.parser')
    else:
        soup = content
        
    images = []
    for img in soup.find_all('img'):
        src = img.get('src', '')
        if src:
            try:
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                if not src.startswith(('http://', 'https://')):
                    src = urljoin(base_url, src)
                
                # ä¸‹è½½å›¾ç‰‡å†…å®¹
                response = requests.get(src)
                if response.status_code == 200:
                    # è·å–å›¾ç‰‡ç±»å‹
                    content_type = response.headers.get('content-type', 'image/jpeg')
                    # è·å–å›¾ç‰‡å†…å®¹å¹¶è¿›è¡Œbase64ç¼–ç 
                    img_data = base64.b64encode(response.content).decode('utf-8')
                    images.append({
                        'src': src,
                        'content_type': content_type,
                        'data': img_data
                    })
                    logger.info(f"æˆåŠŸä¸‹è½½å›¾ç‰‡: {src}")
            except Exception as e:
                logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {src}: {str(e)}")
                continue
    return images

def convert_webp_to_png(image_url, save_dir):
    """å°†webpæ ¼å¼å›¾ç‰‡è½¬æ¢ä¸ºpngæ ¼å¼
    Args:
        image_url: å›¾ç‰‡URL
        save_dir: ä¿å­˜ç›®å½•
    Returns:
        str: è½¬æ¢åçš„å›¾ç‰‡è·¯å¾„ï¼Œå¦‚æœè½¬æ¢å¤±è´¥åˆ™è¿”å›åŸURL
    """
    try:
        logger.info(f"å¼€å§‹å¤„ç†webpå›¾ç‰‡: {image_url}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        logger.info(f"å›¾ç‰‡å°†ä¿å­˜åˆ°: {save_dir}")
        
        # ä¸‹è½½å›¾ç‰‡
        logger.info("æ­£åœ¨ä¸‹è½½å›¾ç‰‡...")
        response = requests.get(image_url)
        if response.status_code != 200:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_url}, çŠ¶æ€ç : {response.status_code}")
            return image_url
            
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        filename = f"img_{uuid.uuid4().hex[:8]}.png"
        save_path = os.path.join(save_dir, filename)
        logger.info(f"ç”Ÿæˆçš„æ–‡ä»¶å: {filename}")
        logger.info(f"å®Œæ•´ä¿å­˜è·¯å¾„: {save_path}")
        
        # è½¬æ¢å›¾ç‰‡æ ¼å¼
        try:
            logger.info("æ­£åœ¨è½¬æ¢å›¾ç‰‡æ ¼å¼...")
            image = Image.open(io.BytesIO(response.content))
            logger.info(f"åŸå§‹å›¾ç‰‡ä¿¡æ¯: æ¨¡å¼={image.mode}, å¤§å°={image.size}")
            
            # å¦‚æœå›¾ç‰‡æœ‰é€æ˜é€šé“ï¼Œä¿ç•™alphaé€šé“
            if image.mode in ('RGBA', 'LA') or (image.mode == 'P' and 'transparency' in image.info):
                logger.info("æ£€æµ‹åˆ°é€æ˜é€šé“ï¼Œä¿ç•™alphaé€šé“")
                image = image.convert('RGBA')
            else:
                logger.info("è½¬æ¢ä¸ºRGBæ¨¡å¼")
                image = image.convert('RGB')
                
            image.save(save_path, 'PNG')
            logger.info(f"âœ… å›¾ç‰‡å·²æˆåŠŸè½¬æ¢å¹¶ä¿å­˜: {save_path}")
            relative_path = os.path.join('images', filename)
            logger.info(f"è¿”å›ç›¸å¯¹è·¯å¾„: {relative_path}")
            return relative_path
            
        except Exception as e:
            logger.error(f"è½¬æ¢å›¾ç‰‡å¤±è´¥: {str(e)}")
            logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            import traceback
            logger.error(traceback.format_exc())
            return image_url
            
    except Exception as e:
        logger.error(f"å¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        import traceback
        logger.error(traceback.format_exc())
        return image_url

def process_images_in_content(content, base_url, save_dir):
    """å¤„ç†æ–‡ç« å†…å®¹ä¸­çš„å›¾ç‰‡
    Args:
        content: BeautifulSoupå¯¹è±¡æˆ–HTMLå­—ç¬¦ä¸²
        base_url: åŸå§‹é¡µé¢çš„URL
        save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    Returns:
        str: å¤„ç†åçš„å†…å®¹
    """
    logger.info("=== å¼€å§‹å¤„ç†æ–‡ç« ä¸­çš„å›¾ç‰‡ ===")
    logger.info(f"åŸºç¡€URL: {base_url}")
    logger.info(f"ä¿å­˜ç›®å½•: {save_dir}")
    
    if isinstance(content, str):
        logger.debug("å°†å­—ç¬¦ä¸²å†…å®¹è½¬æ¢ä¸ºBeautifulSoupå¯¹è±¡")
        soup = BeautifulSoup(content, 'html.parser')
    else:
        soup = content
        
    # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
    # images_dir = os.path.join(save_dir, 'images')
    # os.makedirs(images_dir, exist_ok=True)
    # logger.info(f"å›¾ç‰‡ä¿å­˜ç›®å½•: {images_dir}")
    
    # å¤„ç†æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
    img_count = 0
    webp_count = 0
    for img in soup.find_all('img'):
        img_count += 1
        
        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æºå±æ€§
        src_attrs = ['src', 'data-src', 'data-original-src', 'data-backgroud', 'data-original']
        src = None
        for attr in src_attrs:
            if attr in img.attrs and img[attr]:
                src = img[attr]
                logger.info(f"ä½¿ç”¨å›¾ç‰‡å±æ€§ {attr}: {src}")
                break
                
        if not src:
            logger.warning(f"è·³è¿‡æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡æºçš„æ ‡ç­¾: {img}")
            continue
            
        # å¤„ç†åŒæ–œæ å¼€å¤´çš„URL
        if src.startswith('//'):
            src = 'https:' + src
            logger.info(f"ä¿®æ­£åŒæ–œæ URL: {src}")
            
        img['src'] = src
        logger.info(f"å¤„ç†ç¬¬ {img_count} ä¸ªå›¾ç‰‡: {src}")
            
        # è½¬æ¢ä¸ºç»å¯¹URL
        if not src.startswith(('http://', 'https://')):
            absolute_url = urljoin(base_url, src)
            logger.info(f"è½¬æ¢ç›¸å¯¹URLä¸ºç»å¯¹URL: {src} -> {absolute_url}")
            src = absolute_url
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºwebpæ ¼å¼
        if '.webp' in src.lower() or 'format/webp' in src.lower():
            webp_count += 1
            logger.info(f"ğŸ–¼ï¸ å‘ç°ç¬¬ {webp_count} ä¸ªwebpå›¾ç‰‡: {src}")
            
            # å¦‚æœæ˜¯ç®€ä¹¦çš„å›¾ç‰‡URLï¼Œå°è¯•å»æ‰format/webpå‚æ•°
            if 'jianshu.io' in src and 'format/webp' in src:
                logger.info("æ£€æµ‹åˆ°ç®€ä¹¦å›¾ç‰‡URLï¼Œå°è¯•å»æ‰format/webpå‚æ•°")
                # ç§»é™¤format/webpå‚æ•°ï¼Œä¿ç•™åŸå§‹å›¾ç‰‡æ ¼å¼
                new_src = re.sub(r'\|imageView2/2/w/\d+/format/webp', '', src)
                new_src = re.sub(r'\?.*format/webp', '', new_src)
                logger.info(f"å¤„ç†åçš„URL: {new_src}")
                img['src'] = new_src
                continue
                
            # è½¬æ¢å¹¶ä¿å­˜å›¾ç‰‡
            new_src = convert_webp_to_png(src, images_dir)
            if new_src != src:
                logger.info(f"âœ… æ›´æ–°å›¾ç‰‡é“¾æ¥: {src} -> {new_src}")
                img['src'] = new_src
            else:
                logger.warning("âŒ å›¾ç‰‡è½¬æ¢å¤±è´¥ï¼Œä¿ç•™åŸå§‹é“¾æ¥")
                
    logger.info(f"=== å›¾ç‰‡å¤„ç†å®Œæˆ: å…±å¤„ç† {img_count} ä¸ªå›¾ç‰‡ï¼Œå…¶ä¸­ {webp_count} ä¸ªwebpæ ¼å¼ ===")
    return str(soup)

__all__ = [
    'save_as_html',
    'save_as_pdf',
    'save_as_markdown',
    'save_as_mhtml',
]